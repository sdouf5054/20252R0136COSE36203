{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Report Generator - Full Pipeline (Phase 2)\n",
    "\n",
    "**Version**: 2.0  \n",
    "**Date**: 2025-12-02  \n",
    "**Model**: Llama-3.1-8B (4-bit quantization)  \n",
    "**Environment**: Google Colab (T4 GPU recommended)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Overview\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Phase 1 Quick Demoì˜ ê°œì„  ë²„ì „ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê°œì„ ì‚¬í•­ì„ í¬í•¨í•©ë‹ˆë‹¤:\n",
    "\n",
    "### Key Improvements from Phase 1\n",
    "- âœ… **Multilingual Support**: Llama-3.1-8Bë¡œ ì „í™˜ (Korean í’ˆì§ˆ í–¥ìƒ)\n",
    "- âœ… **Modular Design**: ëª¨ë¸, í”„ë¡¬í”„íŠ¸, ì „ì²˜ë¦¬ ëª¨ë“ˆí™”\n",
    "- âœ… **Extensible Architecture**: Team model í†µí•© ì¤€ë¹„\n",
    "- âœ… **Better Preprocessing**: URL ì œê±°, ì–¸ì–´ ê°ì§€, fallback ë©”ì»¤ë‹ˆì¦˜\n",
    "- âœ… **YouTube API Integration**: ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì˜µì…˜\n",
    "\n",
    "### Phase 1 vs Phase 2\n",
    "\n",
    "| Feature | Phase 1 (Quick Demo) | Phase 2 (Full Pipeline) |\n",
    "|---------|---------------------|------------------------|\n",
    "| Model | BART-CNN (400M) | Llama-3.1-8B (8B) |\n",
    "| Korean Quality | 0-2/10 | 6-8/10 (expected) |\n",
    "| English Quality | 4-8/10 | 7-9/10 (expected) |\n",
    "| Execution Time | 6-9 min | 20-30 min (10 videos) |\n",
    "| Hardware | CPU only | GPU (T4) recommended |\n",
    "| API Keys | None | YouTube API (optional) |\n",
    "| Preprocessing | None | URL removal, language detection |\n",
    "| Team Models | N/A | Ready for integration |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Usage Scenarios\n",
    "\n",
    "### Scenario 1: Sample Data Analysis (No API)\n",
    "```python\n",
    "# Use existing collected data\n",
    "USE_YOUTUBE_API = False\n",
    "data_path = \"full_dataset_20251013_215347.json\"\n",
    "```\n",
    "\n",
    "### Scenario 2: Real-time Collection (With API)\n",
    "```python\n",
    "# Collect fresh data from YouTube\n",
    "USE_YOUTUBE_API = True\n",
    "YOUTUBE_API_KEY = \"your_api_key_here\"\n",
    "```\n",
    "\n",
    "### Scenario 3: Model Experimentation\n",
    "```python\n",
    "# Try different models\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"  # Default\n",
    "# MODEL_NAME = \"google/gemma-2-9b-it\"  # Alternative\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"  # Alternative\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers accelerate bitsandbytes torch\n",
    "!pip install -q langdetect isodate\n",
    "!pip install -q google-api-python-client  # For YouTube API (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import isodate\n",
    "from langdetect import detect, LangDetectException\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. Configuration & Settings\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œ ëª¨ë“  ì„¤ì •ì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    # ===== Model Configuration =====\n",
    "    model_name: str = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    use_4bit: bool = True  # 4-bit quantization (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "    max_new_tokens: int = 512  # ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "    temperature: float = 0.7  # ìƒì„± ë‹¤ì–‘ì„± (ë‚®ì„ìˆ˜ë¡ deterministic)\n",
    "    top_p: float = 0.9  # Nucleus sampling\n",
    "    \n",
    "    # ===== Data Configuration =====\n",
    "    use_youtube_api: bool = False  # YouTube API ì‚¬ìš© ì—¬ë¶€\n",
    "    youtube_api_key: Optional[str] = None  # API key (ì‚¬ìš©ì‹œ)\n",
    "    data_path: str = \"full_dataset_20251013_215347.json\"  # ìƒ˜í”Œ ë°ì´í„° ê²½ë¡œ\n",
    "    \n",
    "    # ===== Processing Configuration =====\n",
    "    max_description_length: int = 2000  # ì„¤ëª… ìµœëŒ€ ê¸¸ì´ (í† í° ì ˆì•½)\n",
    "    max_comments_to_process: int = 50  # ì²˜ë¦¬í•  ìµœëŒ€ ëŒ“ê¸€ ìˆ˜\n",
    "    min_comment_length: int = 10  # ìµœì†Œ ëŒ“ê¸€ ê¸¸ì´ (ì§§ì€ ëŒ“ê¸€ í•„í„°ë§)\n",
    "    remove_urls: bool = True  # URL ì œê±° ì—¬ë¶€\n",
    "    detect_language: bool = True  # ì–¸ì–´ ê°ì§€ ì—¬ë¶€\n",
    "    \n",
    "    # ===== Team Model Integration (Future) =====\n",
    "    use_category_model: bool = False  # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ëª¨ë¸ ì‚¬ìš©\n",
    "    use_sentiment_model: bool = False  # ê°ì • ë¶„ì„ ëª¨ë¸ ì‚¬ìš©\n",
    "    category_model_path: Optional[str] = None\n",
    "    sentiment_model_path: Optional[str] = None\n",
    "    \n",
    "    # ===== Output Configuration =====\n",
    "    output_format: str = \"markdown\"  # markdown or json\n",
    "    save_reports: bool = True  # ë¦¬í¬íŠ¸ ì €ì¥ ì—¬ë¶€\n",
    "    output_dir: str = \"reports\"  # ë¦¬í¬íŠ¸ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"ì„¤ì • ê²€ì¦\"\"\"\n",
    "        if self.use_youtube_api and not self.youtube_api_key:\n",
    "            raise ValueError(\"YouTube APIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ API keyê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        if self.use_category_model and not self.category_model_path:\n",
    "            raise ValueError(\"Category modelì„ ì‚¬ìš©í•˜ë ¤ë©´ model pathê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        if self.use_sentiment_model and not self.sentiment_model_path:\n",
    "            raise ValueError(\"Sentiment modelì„ ì‚¬ìš©í•˜ë ¤ë©´ model pathê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ì„¤ì • ìƒì„±\n",
    "config = PipelineConfig(\n",
    "    # ê¸°ë³¸ ì„¤ì •: ìƒ˜í”Œ ë°ì´í„°ë¡œ Llama-3.1-8B ì‚¬ìš©\n",
    "    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    use_4bit=True,\n",
    "    use_youtube_api=False,\n",
    "    data_path=\"full_dataset_20251013_215347.json\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"4-bit Quantization: {config.use_4bit}\")\n",
    "print(f\"Temperature: {config.temperature}\")\n",
    "print(f\"Max New Tokens: {config.max_new_tokens}\")\n",
    "print(f\"YouTube API: {config.use_youtube_api}\")\n",
    "print(f\"Data Source: {config.data_path if not config.use_youtube_api else 'YouTube API'}\")\n",
    "print(f\"Category Model: {config.use_category_model}\")\n",
    "print(f\"Sentiment Model: {config.use_sentiment_model}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§© 3. Prompt Templates\n",
    "\n",
    "í”„ë¡¬í”„íŠ¸ë¥¼ ììœ ë¡­ê²Œ ìˆ˜ì •í•˜ì—¬ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplates:\n",
    "    \"\"\"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    # ===== Video Summary Prompt =====\n",
    "    VIDEO_SUMMARY_SYSTEM = \"\"\"You are an expert content analyst specializing in YouTube video analysis.\n",
    "Your task is to create concise, informative summaries of videos based on their metadata.\n",
    "Focus on accuracy and avoid hallucinations. If information is unclear, acknowledge uncertainty.\"\"\"\n",
    "    \n",
    "    VIDEO_SUMMARY_USER = \"\"\"Analyze this YouTube video and create a concise summary.\n",
    "\n",
    "Video Information:\n",
    "- Title: {title}\n",
    "- Description: {description}\n",
    "- Category: {category}\n",
    "- Duration: {duration} seconds\n",
    "- Channel: {channel}\n",
    "\n",
    "Instructions:\n",
    "1. Write a 3-5 sentence summary in {language}\n",
    "2. Focus on what the video is about, its purpose, and key highlights\n",
    "3. Do NOT make up information not present in the metadata\n",
    "4. If the description is minimal, acknowledge this limitation\n",
    "5. Be factual and avoid speculative language\n",
    "\n",
    "Summary:\"\"\"\n",
    "    \n",
    "    # ===== Reaction Summary Prompt =====\n",
    "    REACTION_SUMMARY_SYSTEM = \"\"\"You are an expert in social media sentiment analysis.\n",
    "Your task is to summarize audience reactions based on YouTube comments.\n",
    "Identify common themes, sentiments, and notable opinions.\"\"\"\n",
    "    \n",
    "    REACTION_SUMMARY_USER = \"\"\"Analyze these YouTube comments and summarize the audience reaction.\n",
    "\n",
    "Video: {title}\n",
    "\n",
    "Top Comments (sorted by likes):\n",
    "{comments}\n",
    "\n",
    "Instructions:\n",
    "1. Write a 3-5 sentence summary in {language}\n",
    "2. Identify overall sentiment (positive, negative, mixed)\n",
    "3. Highlight common themes or recurring topics\n",
    "4. Mention any notable or particularly insightful comments\n",
    "5. Be objective and balanced in your analysis\n",
    "\n",
    "Audience Reaction Summary:\"\"\"\n",
    "    \n",
    "    # ===== Custom Prompt (User can modify) =====\n",
    "    CUSTOM_PROMPT = None  # Set this to override default prompts\n",
    "    \n",
    "    @classmethod\n",
    "    def get_video_summary_prompt(cls, video_info: Dict, language: str = \"Korean\") -> List[Dict]:\n",
    "        \"\"\"ë¹„ë””ì˜¤ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "        if cls.CUSTOM_PROMPT:\n",
    "            return cls.CUSTOM_PROMPT\n",
    "        \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": cls.VIDEO_SUMMARY_SYSTEM},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": cls.VIDEO_SUMMARY_USER.format(\n",
    "                    title=video_info.get('title', 'N/A'),\n",
    "                    description=video_info.get('description', 'N/A')[:2000],\n",
    "                    category=video_info.get('category_name', 'N/A'),\n",
    "                    duration=video_info.get('duration_seconds', 'N/A'),\n",
    "                    channel=video_info.get('channel_title', 'N/A'),\n",
    "                    language=language\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_reaction_summary_prompt(cls, title: str, comments: str, language: str = \"Korean\") -> List[Dict]:\n",
    "        \"\"\"ë°˜ì‘ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "        if cls.CUSTOM_PROMPT:\n",
    "            return cls.CUSTOM_PROMPT\n",
    "        \n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": cls.REACTION_SUMMARY_SYSTEM},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": cls.REACTION_SUMMARY_USER.format(\n",
    "                    title=title,\n",
    "                    comments=comments,\n",
    "                    language=language\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "\n",
    "print(\"âœ… Prompt templates loaded\")\n",
    "print(\"\\nğŸ’¡ Tip: í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ë ¤ë©´ PromptTemplates í´ë˜ìŠ¤ì˜ í…œí”Œë¦¿ì„ í¸ì§‘í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ 4. Preprocessing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    def remove_urls(self, text: str) -> str:\n",
    "        \"\"\"URL ì œê±°\"\"\"\n",
    "        return self.url_pattern.sub('', text)\n",
    "    \n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"ì–¸ì–´ ê°ì§€\"\"\"\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "            # Map language codes to full names\n",
    "            lang_map = {\n",
    "                'ko': 'Korean',\n",
    "                'en': 'English',\n",
    "                'ja': 'Japanese',\n",
    "                'zh-cn': 'Chinese',\n",
    "                'zh-tw': 'Chinese'\n",
    "            }\n",
    "            return lang_map.get(lang, 'English')\n",
    "        except LangDetectException:\n",
    "            return 'English'  # Default fallback\n",
    "    \n",
    "    def clean_description(self, description: str) -> str:\n",
    "        \"\"\"ì„¤ëª… ì •ë¦¬\"\"\"\n",
    "        if not description:\n",
    "            return \"No description available.\"\n",
    "        \n",
    "        # Remove URLs if configured\n",
    "        if self.config.remove_urls:\n",
    "            description = self.remove_urls(description)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        description = ' '.join(description.split())\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(description) > self.config.max_description_length:\n",
    "            description = description[:self.config.max_description_length] + \"...\"\n",
    "        \n",
    "        # Check if description is meaningful\n",
    "        if len(description.strip()) < 20:\n",
    "            return \"Minimal description available.\"\n",
    "        \n",
    "        return description\n",
    "    \n",
    "    def filter_comments(self, comments: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"ëŒ“ê¸€ í•„í„°ë§ ë° ì •ë ¬\"\"\"\n",
    "        # Filter out short comments\n",
    "        filtered = [\n",
    "            c for c in comments \n",
    "            if len(c.get('text', '')) >= self.config.min_comment_length\n",
    "        ]\n",
    "        \n",
    "        # Sort by like_count (descending)\n",
    "        filtered.sort(key=lambda x: x.get('like_count', 0), reverse=True)\n",
    "        \n",
    "        # Limit to max_comments_to_process\n",
    "        return filtered[:self.config.max_comments_to_process]\n",
    "    \n",
    "    def format_comments_for_prompt(self, comments: List[Dict]) -> str:\n",
    "        \"\"\"í”„ë¡¬í”„íŠ¸ìš© ëŒ“ê¸€ í¬ë§·íŒ…\"\"\"\n",
    "        if not comments:\n",
    "            return \"No comments available.\"\n",
    "        \n",
    "        formatted = []\n",
    "        for i, comment in enumerate(comments, 1):\n",
    "            text = comment.get('text', '')\n",
    "            likes = comment.get('like_count', 0)\n",
    "            \n",
    "            # Clean text\n",
    "            if self.config.remove_urls:\n",
    "                text = self.remove_urls(text)\n",
    "            \n",
    "            formatted.append(f\"{i}. [{likes} likes] {text}\")\n",
    "        \n",
    "        return \"\\n\".join(formatted)\n",
    "    \n",
    "    def parse_duration(self, duration_str: str) -> int:\n",
    "        \"\"\"ISO 8601 durationì„ ì´ˆë¡œ ë³€í™˜\"\"\"\n",
    "        try:\n",
    "            duration = isodate.parse_duration(duration_str)\n",
    "            return int(duration.total_seconds())\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "# ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”\n",
    "preprocessor = TextPreprocessor(config)\n",
    "print(\"âœ… Text preprocessor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– 5. Model Loading\n",
    "\n",
    "Llama-3.1-8B ëª¨ë¸ì„ 4-bit quantizationìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"LLM ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipe = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        print(f\"\\nğŸ”„ Loading model: {self.config.model_name}\")\n",
    "        print(f\"âš™ï¸ 4-bit quantization: {self.config.use_4bit}\")\n",
    "        \n",
    "        # Tokenizer ë¡œë“œ\n",
    "        print(\"Loading tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # 4-bit quantization ì„¤ì •\n",
    "        if self.config.use_4bit:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "        else:\n",
    "            quantization_config = None\n",
    "        \n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        print(\"Loading model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16 if not self.config.use_4bit else None\n",
    "        )\n",
    "        \n",
    "        # Pipeline ìƒì„±\n",
    "        print(\"Creating pipeline...\")\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            max_new_tokens=self.config.max_new_tokens,\n",
    "            temperature=self.config.temperature,\n",
    "            top_p=self.config.top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Model loaded successfully!\")\n",
    "        print(f\"ğŸ“Š Model size: ~{self.get_model_size_gb():.2f} GB\")\n",
    "    \n",
    "    def get_model_size_gb(self) -> float:\n",
    "        \"\"\"ëª¨ë¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\"\"\"\n",
    "        if self.model is None:\n",
    "            return 0\n",
    "        \n",
    "        param_size = sum(p.nelement() * p.element_size() for p in self.model.parameters())\n",
    "        buffer_size = sum(b.nelement() * b.element_size() for b in self.model.buffers())\n",
    "        total_size_gb = (param_size + buffer_size) / 1024**3\n",
    "        return total_size_gb\n",
    "    \n",
    "    def generate(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
    "        if self.pipe is None:\n",
    "            raise RuntimeError(\"Model not loaded. Call load_model() first.\")\n",
    "        \n",
    "        # Generate\n",
    "        outputs = self.pipe(messages)\n",
    "        \n",
    "        # Extract generated text\n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "        \n",
    "        # Return only the assistant's response\n",
    "        if isinstance(generated_text, list):\n",
    "            return generated_text[-1][\"content\"]\n",
    "        else:\n",
    "            return generated_text\n",
    "\n",
    "# ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™” ë° ë¡œë“œ\n",
    "model_manager = ModelManager(config)\n",
    "model_manager.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Œ 6. Team Model Integration (Optional)\n",
    "\n",
    "íŒ€ì›ì´ ê°œë°œí•œ ëª¨ë¸ì„ í†µí•©í•  ì¤€ë¹„ê°€ ëœ êµ¬ì¡°ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeamModelIntegration:\n",
    "    \"\"\"íŒ€ ëª¨ë¸ í†µí•©ì„ ìœ„í•œ ì¸í„°í˜ì´ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.category_model = None\n",
    "        self.sentiment_model = None\n",
    "    \n",
    "    def load_category_model(self):\n",
    "        \"\"\"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ (íŒ€ì› êµ¬í˜„ ëŒ€ê¸°)\"\"\"\n",
    "        if not self.config.use_category_model:\n",
    "            return\n",
    "        \n",
    "        # TODO: íŒ€ì›ì´ ì œê³µí•˜ëŠ” ëª¨ë¸ ë¡œë“œ ì½”ë“œ\n",
    "        # Example:\n",
    "        # from team_models import CategoryClassifier\n",
    "        # self.category_model = CategoryClassifier.load(self.config.category_model_path)\n",
    "        \n",
    "        print(\"âš ï¸ Category model not implemented yet\")\n",
    "    \n",
    "    def load_sentiment_model(self):\n",
    "        \"\"\"ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ (íŒ€ì› êµ¬í˜„ ëŒ€ê¸°)\"\"\"\n",
    "        if not self.config.use_sentiment_model:\n",
    "            return\n",
    "        \n",
    "        # TODO: íŒ€ì›ì´ ì œê³µí•˜ëŠ” ëª¨ë¸ ë¡œë“œ ì½”ë“œ\n",
    "        # Example:\n",
    "        # from team_models import SentimentAnalyzer\n",
    "        # self.sentiment_model = SentimentAnalyzer.load(self.config.sentiment_model_path)\n",
    "        \n",
    "        print(\"âš ï¸ Sentiment model not implemented yet\")\n",
    "    \n",
    "    def predict_category(self, video_info: Dict) -> Optional[str]:\n",
    "        \"\"\"ë¹„ë””ì˜¤ ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡\"\"\"\n",
    "        if not self.config.use_category_model or self.category_model is None:\n",
    "            return None\n",
    "        \n",
    "        # TODO: ëª¨ë¸ inference\n",
    "        # return self.category_model.predict(video_info)\n",
    "        return None\n",
    "    \n",
    "    def analyze_sentiment(self, comments: List[Dict]) -> Optional[Dict]:\n",
    "        \"\"\"ëŒ“ê¸€ ê°ì • ë¶„ì„\"\"\"\n",
    "        if not self.config.use_sentiment_model or self.sentiment_model is None:\n",
    "            return None\n",
    "        \n",
    "        # TODO: ëª¨ë¸ inference\n",
    "        # return self.sentiment_model.analyze(comments)\n",
    "        return None\n",
    "\n",
    "# íŒ€ ëª¨ë¸ í†µí•© ì¤€ë¹„\n",
    "team_models = TeamModelIntegration(config)\n",
    "team_models.load_category_model()\n",
    "team_models.load_sentiment_model()\n",
    "\n",
    "print(\"âœ… Team model integration interface ready\")\n",
    "print(\"ğŸ’¡ Tip: íŒ€ì›ì˜ ëª¨ë¸ì´ ì¤€ë¹„ë˜ë©´ load_category_model()ê³¼ load_sentiment_model()ì„ êµ¬í˜„í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ 7. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"ë°ì´í„° ë¡œë”© í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def load_from_file(self, file_path: str) -> List[Dict]:\n",
    "        \"\"\"JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "        print(f\"\\nğŸ“‚ Loading data from: {file_path}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(data)} videos\")\n",
    "        return data\n",
    "    \n",
    "    def load_from_youtube_api(self, video_ids: List[str]) -> List[Dict]:\n",
    "        \"\"\"YouTube APIì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘\"\"\"\n",
    "        if not self.config.use_youtube_api:\n",
    "            raise RuntimeError(\"YouTube API is not enabled in config\")\n",
    "        \n",
    "        # TODO: YouTube API í†µí•©\n",
    "        # ê¸°ì¡´ youtube_auto_collection_251013.ipynbì˜ í•¨ìˆ˜ë“¤ì„ ì¬ì‚¬ìš©\n",
    "        # Example:\n",
    "        # from youtube_collector import get_video_details, get_video_comments\n",
    "        # ...\n",
    "        \n",
    "        print(\"âš ï¸ YouTube API collection not implemented yet\")\n",
    "        print(\"ğŸ’¡ Use load_from_file() with existing data for now\")\n",
    "        return []\n",
    "    \n",
    "    def get_data(self) -> List[Dict]:\n",
    "        \"\"\"ì„¤ì •ì— ë”°ë¼ ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "        if self.config.use_youtube_api:\n",
    "            # TODO: Implement API collection\n",
    "            raise NotImplementedError(\"YouTube API collection not implemented yet\")\n",
    "        else:\n",
    "            return self.load_from_file(self.config.data_path)\n",
    "\n",
    "# ë°ì´í„° ë¡œë” ì´ˆê¸°í™”\n",
    "data_loader = DataLoader(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¬ 8. Report Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGenerator:\n",
    "    \"\"\"ë¦¬í¬íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 config: PipelineConfig,\n",
    "                 model_manager: ModelManager,\n",
    "                 preprocessor: TextPreprocessor,\n",
    "                 team_models: TeamModelIntegration):\n",
    "        self.config = config\n",
    "        self.model_manager = model_manager\n",
    "        self.preprocessor = preprocessor\n",
    "        self.team_models = team_models\n",
    "    \n",
    "    def generate_video_summary(self, video_info: Dict) -> str:\n",
    "        \"\"\"ë¹„ë””ì˜¤ ìš”ì•½ ìƒì„±\"\"\"\n",
    "        # Preprocess\n",
    "        video_info['description'] = self.preprocessor.clean_description(\n",
    "            video_info.get('description', '')\n",
    "        )\n",
    "        video_info['duration_seconds'] = self.preprocessor.parse_duration(\n",
    "            video_info.get('duration', 'PT0S')\n",
    "        )\n",
    "        \n",
    "        # Detect language\n",
    "        language = \"Korean\"  # Default\n",
    "        if self.config.detect_language:\n",
    "            title_lang = self.preprocessor.detect_language(video_info.get('title', ''))\n",
    "            desc_lang = self.preprocessor.detect_language(video_info.get('description', ''))\n",
    "            language = title_lang if title_lang != 'English' else desc_lang\n",
    "        \n",
    "        # Generate prompt\n",
    "        messages = PromptTemplates.get_video_summary_prompt(video_info, language)\n",
    "        \n",
    "        # Generate summary\n",
    "        try:\n",
    "            summary = self.model_manager.generate(messages)\n",
    "            return summary.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error generating video summary: {e}\")\n",
    "            return \"Summary generation failed. Please check the error logs.\"\n",
    "    \n",
    "    def generate_reaction_summary(self, title: str, comments: List[Dict]) -> str:\n",
    "        \"\"\"ë°˜ì‘ ìš”ì•½ ìƒì„±\"\"\"\n",
    "        # Filter and format comments\n",
    "        filtered_comments = self.preprocessor.filter_comments(comments)\n",
    "        \n",
    "        if not filtered_comments:\n",
    "            return \"No comments available for analysis.\"\n",
    "        \n",
    "        comments_text = self.preprocessor.format_comments_for_prompt(filtered_comments)\n",
    "        \n",
    "        # Detect language from comments\n",
    "        language = \"Korean\"  # Default\n",
    "        if self.config.detect_language and filtered_comments:\n",
    "            sample_text = \" \".join([c.get('text', '') for c in filtered_comments[:5]])\n",
    "            language = self.preprocessor.detect_language(sample_text)\n",
    "        \n",
    "        # Generate prompt\n",
    "        messages = PromptTemplates.get_reaction_summary_prompt(title, comments_text, language)\n",
    "        \n",
    "        # Generate summary\n",
    "        try:\n",
    "            summary = self.model_manager.generate(messages)\n",
    "            return summary.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error generating reaction summary: {e}\")\n",
    "            return \"Reaction summary generation failed. Please check the error logs.\"\n",
    "    \n",
    "    def calculate_engagement_metrics(self, video_info: Dict) -> Dict:\n",
    "        \"\"\"ì°¸ì—¬ë„ ì§€í‘œ ê³„ì‚°\"\"\"\n",
    "        views = video_info.get('view_count', 0)\n",
    "        likes = video_info.get('like_count', 0)\n",
    "        comments = video_info.get('comment_count', 0)\n",
    "        \n",
    "        # Engagement rate\n",
    "        engagement_rate = ((likes + comments) / views * 100) if views > 0 else 0\n",
    "        \n",
    "        # Like rate\n",
    "        like_rate = (likes / views * 100) if views > 0 else 0\n",
    "        \n",
    "        # Comment rate\n",
    "        comment_rate = (comments / views * 100) if views > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'views': views,\n",
    "            'likes': likes,\n",
    "            'comments': comments,\n",
    "            'engagement_rate': round(engagement_rate, 2),\n",
    "            'like_rate': round(like_rate, 2),\n",
    "            'comment_rate': round(comment_rate, 2)\n",
    "        }\n",
    "    \n",
    "    def format_markdown_report(self, video_data: Dict, \n",
    "                               video_summary: str, \n",
    "                               reaction_summary: str,\n",
    "                               metrics: Dict,\n",
    "                               team_predictions: Dict = None) -> str:\n",
    "        \"\"\"Markdown í˜•ì‹ì˜ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        video_info = video_data['video_info']\n",
    "        comments = video_data.get('comments', [])\n",
    "        \n",
    "        # Header\n",
    "        report = f\"\"\"# YouTube Video Report\n",
    "\n",
    "**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Model**: {self.config.model_name}\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¹ Video Information\n",
    "\n",
    "- **Title**: {video_info.get('title', 'N/A')}\n",
    "- **Channel**: {video_info.get('channel_title', 'N/A')}\n",
    "- **Category**: {video_info.get('category_name', 'N/A')}\n",
    "- **Published**: {video_info.get('published_at', 'N/A')}\n",
    "- **Duration**: {self.preprocessor.parse_duration(video_info.get('duration', 'PT0S'))} seconds\n",
    "- **Video ID**: `{video_info.get('video_id', 'N/A')}`\n",
    "- **URL**: https://www.youtube.com/watch?v={video_info.get('video_id', '')}\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Engagement Metrics\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Views | {metrics['views']:,} |\n",
    "| Likes | {metrics['likes']:,} |\n",
    "| Comments | {metrics['comments']:,} |\n",
    "| Engagement Rate | {metrics['engagement_rate']}% |\n",
    "| Like Rate | {metrics['like_rate']}% |\n",
    "| Comment Rate | {metrics['comment_rate']}% |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Video Summary\n",
    "\n",
    "{video_summary}\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¬ Audience Reaction Summary\n",
    "\n",
    "{reaction_summary}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "        \n",
    "        # Team model predictions (if available)\n",
    "        if team_predictions:\n",
    "            report += \"\"\"## ğŸ¤– Team Model Predictions\n",
    "\n",
    "\"\"\"\n",
    "            if 'category' in team_predictions:\n",
    "                report += f\"- **Predicted Category**: {team_predictions['category']}\\n\"\n",
    "            \n",
    "            if 'sentiment' in team_predictions:\n",
    "                sentiment = team_predictions['sentiment']\n",
    "                report += f\"\"\"- **Sentiment Analysis**:\n",
    "  - Positive: {sentiment.get('positive', 0):.1%}\n",
    "  - Neutral: {sentiment.get('neutral', 0):.1%}\n",
    "  - Negative: {sentiment.get('negative', 0):.1%}\n",
    "\"\"\"\n",
    "            report += \"\\n---\\n\\n\"\n",
    "        \n",
    "        # Top comments\n",
    "        top_comments = self.preprocessor.filter_comments(comments)[:5]\n",
    "        if top_comments:\n",
    "            report += \"\"\"## ğŸ” Top Comments\n",
    "\n",
    "\"\"\"\n",
    "            for i, comment in enumerate(top_comments, 1):\n",
    "                text = comment.get('text', '')[:200]  # Truncate long comments\n",
    "                likes = comment.get('like_count', 0)\n",
    "                author = comment.get('author', 'Anonymous')\n",
    "                report += f\"{i}. **{author}** ({likes} likes): {text}...\\n\\n\"\n",
    "            \n",
    "            report += \"---\\n\\n\"\n",
    "        \n",
    "        # Footer\n",
    "        report += \"\"\"## ğŸ“Œ Notes\n",
    "\n",
    "- This report was automatically generated using LLM-based analysis\n",
    "- Summaries are based on video metadata and top comments\n",
    "- Quality may vary depending on source data availability\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by YouTube Report Generator - Phase 2 Full Pipeline*\n",
    "\"\"\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def generate_report(self, video_data: Dict) -> str:\n",
    "        \"\"\"ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"\n",
    "        video_info = video_data['video_info']\n",
    "        comments = video_data.get('comments', [])\n",
    "        \n",
    "        print(f\"\\nğŸ¬ Processing: {video_info.get('title', 'Unknown')}\")\n",
    "        \n",
    "        # Generate summaries\n",
    "        print(\"  ğŸ“ Generating video summary...\")\n",
    "        video_summary = self.generate_video_summary(video_info)\n",
    "        \n",
    "        print(\"  ğŸ’¬ Generating reaction summary...\")\n",
    "        reaction_summary = self.generate_reaction_summary(\n",
    "            video_info.get('title', ''), \n",
    "            comments\n",
    "        )\n",
    "        \n",
    "        # Calculate metrics\n",
    "        print(\"  ğŸ“Š Calculating metrics...\")\n",
    "        metrics = self.calculate_engagement_metrics(video_info)\n",
    "        \n",
    "        # Team model predictions (if available)\n",
    "        team_predictions = {}\n",
    "        if self.config.use_category_model:\n",
    "            predicted_category = self.team_models.predict_category(video_info)\n",
    "            if predicted_category:\n",
    "                team_predictions['category'] = predicted_category\n",
    "        \n",
    "        if self.config.use_sentiment_model:\n",
    "            sentiment_analysis = self.team_models.analyze_sentiment(comments)\n",
    "            if sentiment_analysis:\n",
    "                team_predictions['sentiment'] = sentiment_analysis\n",
    "        \n",
    "        # Format report\n",
    "        print(\"  ğŸ“„ Formatting report...\")\n",
    "        report = self.format_markdown_report(\n",
    "            video_data,\n",
    "            video_summary,\n",
    "            reaction_summary,\n",
    "            metrics,\n",
    "            team_predictions if team_predictions else None\n",
    "        )\n",
    "        \n",
    "        print(\"  âœ… Report generated!\")\n",
    "        return report\n",
    "\n",
    "# ë¦¬í¬íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”\n",
    "report_generator = ReportGenerator(config, model_manager, preprocessor, team_models)\n",
    "print(\"âœ… Report generator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 9. Run Pipeline\n",
    "\n",
    "ì´ì œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "dataset = data_loader.get_data()\n",
    "\n",
    "# ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ì„ íƒ (í…ŒìŠ¤íŠ¸: ì²˜ìŒ 3ê°œ)\n",
    "videos_to_process = dataset[:3]  # ì „ì²´: dataset\n",
    "\n",
    "print(f\"\\nğŸ¯ Processing {len(videos_to_process)} videos...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¦¬í¬íŠ¸ ìƒì„±\n",
    "reports = []\n",
    "\n",
    "for i, video_data in enumerate(videos_to_process, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing video {i}/{len(videos_to_process)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        report = report_generator.generate_report(video_data)\n",
    "        reports.append({\n",
    "            'video_id': video_data['video_info']['video_id'],\n",
    "            'report': report\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing video: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… Processed {len(reports)}/{len(videos_to_process)} videos successfully\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 10. Save Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "if config.save_reports:\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saving reports to: {config.output_dir}/\")\n",
    "    \n",
    "    for report_data in reports:\n",
    "        video_id = report_data['video_id']\n",
    "        report = report_data['report']\n",
    "        \n",
    "        # íŒŒì¼ëª… ìƒì„±\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{config.output_dir}/report_{video_id}_{timestamp}.md\"\n",
    "        \n",
    "        # ì €ì¥\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(f\"  âœ… Saved: {filename}\")\n",
    "    \n",
    "    print(f\"\\nâœ… All reports saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 11. Display Sample Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ë¦¬í¬íŠ¸ ì¶œë ¥\n",
    "if reports:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sample Report (First Video)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    print(reports[0]['report'])\nelse:\n",
    "    print(\"âš ï¸ No reports generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 12. Usage Guide & Tips\n",
    "\n",
    "### ëª¨ë¸ ë³€ê²½í•˜ê¸°\n",
    "\n",
    "```python\n",
    "# ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©\n",
    "config = PipelineConfig(\n",
    "    model_name=\"google/gemma-2-9b-it\",  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸\n",
    "    use_4bit=True\n",
    ")\n",
    "```\n",
    "\n",
    "### í”„ë¡¬í”„íŠ¸ ì»¤ìŠ¤í„°ë§ˆì´ì§•\n",
    "\n",
    "```python\n",
    "# PromptTemplates í´ë˜ìŠ¤ì˜ í…œí”Œë¦¿ì„ ì§ì ‘ ìˆ˜ì •\n",
    "PromptTemplates.VIDEO_SUMMARY_USER = \"\"\"Your custom prompt here...\"\"\"\n",
    "```\n",
    "\n",
    "### íŒ€ ëª¨ë¸ í†µí•©\n",
    "\n",
    "```python\n",
    "# ëª¨ë¸ ê²½ë¡œ ì„¤ì •\n",
    "config = PipelineConfig(\n",
    "    use_category_model=True,\n",
    "    category_model_path=\"path/to/model\",\n",
    "    use_sentiment_model=True,\n",
    "    sentiment_model_path=\"path/to/model\"\n",
    ")\n",
    "\n",
    "# TeamModelIntegration í´ë˜ìŠ¤ì˜ ë©”ì„œë“œ êµ¬í˜„\n",
    "# load_category_model() ë° load_sentiment_model()\n",
    "```\n",
    "\n",
    "### ì „ì²˜ë¦¬ ì„¤ì • ì¡°ì •\n",
    "\n",
    "```python\n",
    "config = PipelineConfig(\n",
    "    max_comments_to_process=100,  # ë” ë§ì€ ëŒ“ê¸€ ì²˜ë¦¬\n",
    "    min_comment_length=20,  # ìµœì†Œ ê¸¸ì´ ì¦ê°€\n",
    "    remove_urls=True,  # URL ì œê±° í™œì„±í™”\n",
    ")\n",
    "```\n",
    "\n",
    "### ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •\n",
    "\n",
    "```python\n",
    "config = PipelineConfig(\n",
    "    temperature=0.5,  # ë” ê²°ì •ì ì¸ ì¶œë ¥\n",
    "    max_new_tokens=1024,  # ë” ê¸´ ì¶œë ¥\n",
    "    top_p=0.95  # Sampling íŒŒë¼ë¯¸í„°\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Expected Performance\n",
    "\n",
    "### Execution Time (10 videos)\n",
    "- Model loading: ~3-5 minutes (first time)\n",
    "- Per video processing: ~2-3 minutes\n",
    "- **Total**: ~20-30 minutes\n",
    "\n",
    "### Quality (Expected)\n",
    "- Korean summaries: **6-8/10** (vs 0-2/10 in Phase 1)\n",
    "- English summaries: **7-9/10** (vs 4-8/10 in Phase 1)\n",
    "- Reaction analysis: **7-8/10** (vs 2.7/10 in Phase 1)\n",
    "\n",
    "### Memory Usage\n",
    "- 4-bit model: ~4-5 GB GPU RAM\n",
    "- Without quantization: ~14-16 GB GPU RAM\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Next Steps\n",
    "\n",
    "1. **Test with different models**: Try Gemma-2-9B, Mistral-7B, etc.\n",
    "2. **Optimize prompts**: Experiment with different prompt templates\n",
    "3. **Integrate team models**: Add category classification and sentiment analysis\n",
    "4. **Add YouTube API**: Implement real-time data collection\n",
    "5. **Evaluate quality**: Compare outputs with Phase 1 and ground truth\n",
    "6. **Scale up**: Process entire dataset (20 videos)\n",
    "\n",
    "---\n",
    "\n",
    "**Pipeline Status**: âœ… Ready to use  \n",
    "**Phase**: 2 (Full Pipeline)  \n",
    "**Version**: 2.0  \n",
    "**Last Updated**: 2025-12-02\n",
    "\"\"\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.12\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n",
    "