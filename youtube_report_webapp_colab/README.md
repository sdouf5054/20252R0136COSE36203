# ğŸ¤– YouTube Report Generator (Colab + GPU + LLM Edition)

A web application that generates **AI-powered** comprehensive reports from YouTube videos using open-source LLMs.

> **This version is optimized for Google Colab with GPU runtime.**
> 
> **ğŸ†• NEW: ìŠ¹ì¸ ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í”„ë¦¬ì…‹ ì§€ì›!**

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)
![Gradio](https://img.shields.io/badge/Gradio-4.0+-orange.svg)
![LLM](https://img.shields.io/badge/LLM-Open_Source-green.svg)

## âœ¨ Features

- ğŸ¤– **Real AI Summaries**: Uses open-source LLMs for video content analysis
- ğŸ”“ **No Approval Required**: Mistral, Qwen, Gemma, Phi ë“± ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
- ğŸ”— **URL Input**: Paste any YouTube URL to analyze
- ğŸ“º **Video Preview**: See thumbnail, title, and metadata
- ğŸ“Š **Engagement Metrics**: Views, likes, comments, engagement rates
- ğŸ“ **AI Video Summary**: LLM-generated summary with KEY POINT + DETAILED SUMMARY
- ğŸ’¬ **AI Reaction Analysis**: LLM-analyzed audience sentiment and themes
- ğŸŒ **Public Demo URL**: Share via ngrok (Streamlit) or gradio.live (Gradio)
- âš™ï¸ **Tunable Parameters**: Adjust temperature, tokens, quality gate in UI

---

## ğŸ“¦ Available Model Presets

### ğŸŸ¢ No Approval Required (ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥)

| Preset | Model | VRAM | íŠ¹ì§• |
|--------|-------|------|------|
| `mistral-7b` | Mistral-7B-Instruct-v0.3 | ~5GB | **ì¶”ì²œ** - ë¹ ë¥´ê³  í’ˆì§ˆ ì¢‹ìŒ |
| `qwen2.5-7b` | Qwen2.5-7B-Instruct | ~5GB | **í•œêµ­ì–´ ì¶”ì²œ** - ë‹¤êµ­ì–´ ìš°ìˆ˜ |
| `gemma2-9b` | Gemma-2-9B-it | ~6GB | ê³ í’ˆì§ˆ, ì•½ê°„ ëŠë¦¼ |
| `phi3-mini` | Phi-3-mini-4k-instruct | ~3GB | ê°€ë³ê³  ë¹ ë¦„ |
| `phi3.5-mini` | Phi-3.5-mini-instruct | ~3.5GB | ìµœì‹ , ì„±ëŠ¥ ê°œì„  |
| `tinyllama` | TinyLlama-1.1B-Chat | ~2.5GB | ë§¤ìš° ê°€ë²¼ì›€ |
| `stablelm-2` | StableLM-2-1.6B-Chat | ~3GB | ê°€ë³ê³  ì•ˆì •ì  |

### ğŸŸ¡ Requires HF Approval (ìŠ¹ì¸ í•„ìš”)

| Preset | Model | ì‹ ì²­ ë§í¬ |
|--------|-------|----------|
| `llama3.1-8b` | Llama-3.1-8B-Instruct | [ì‹ ì²­](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) |
| `llama3.2-3b` | Llama-3.2-3B-Instruct | [ì‹ ì²­](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) |

---

## ğŸš€ Quick Start (Google Colab)

### Option 1: Gradio (Easiest - Built-in Public URL)

```python
# Cell 1: Install dependencies
!pip install gradio google-api-python-client isodate langdetect
!pip install transformers accelerate bitsandbytes

# Cell 2: Set API key
import os
os.environ['YOUTUBE_API_KEY'] = 'YOUR_YOUTUBE_API_KEY'

# Cell 3: Upload files
# Upload pipeline.py and app_gradio.py using Colab file browser

# Cell 4: (ì„ íƒ) ëª¨ë¸ í™•ì¸
from pipeline import list_available_models
list_available_models()

# Cell 5: Run (automatic gradio.live URL!)
!python app_gradio.py
```

### ğŸ”§ ëª¨ë¸ ë³€ê²½í•˜ê¸° (Colab ì…€ì—ì„œ)

```python
# Cell: ëª¨ë¸ ë³€ê²½ í›„ ì•± ì‹¤í–‰
from pipeline import PipelineConfig, MODEL_PRESETS, list_available_models

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë³´ê¸°
list_available_models()

# ë°©ë²• 1: í”„ë¦¬ì…‹ ì‚¬ìš© (ì¶”ì²œ)
config = PipelineConfig.from_preset('qwen2.5-7b')  # í•œêµ­ì–´ ì¶”ì²œ
config = PipelineConfig.from_preset('mistral-7b')  # ì˜ì–´ ì¶”ì²œ
config = PipelineConfig.from_preset('phi3-mini')   # ê°€ë²¼ìš´ ëª¨ë¸

# ë°©ë²• 2: ì§ì ‘ ëª¨ë¸ ì§€ì •
config = PipelineConfig(
    model_name="mistralai/Mistral-7B-Instruct-v0.3",
    use_4bit=True,
    max_new_tokens=512
)

# ë°©ë²• 3: í”„ë¦¬ì…‹ + ì»¤ìŠ¤í…€ ì„¤ì •
config = PipelineConfig.from_preset('qwen2.5-7b', max_new_tokens=256, temperature=0.5)

# ì´í›„ ModelManagerì— ì „ë‹¬
from pipeline import ModelManager
model_manager = ModelManager(config)
model_manager.load_model()
```

### Gradio ì•±ì—ì„œ ëª¨ë¸ ë³€ê²½

```python
# app_gradio.py ì‹¤í–‰ ì „ì— í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë¸ ì§€ì •
import os
os.environ['YOUTUBE_API_KEY'] = 'YOUR_KEY'
os.environ['MODEL_PRESET'] = 'qwen2.5-7b'  # í”„ë¦¬ì…‹ ì´ë¦„
# ë˜ëŠ”
os.environ['MODEL_NAME'] = 'microsoft/Phi-3-mini-4k-instruct'  # ì§ì ‘ ì§€ì •

!python app_gradio.py
```

---

### Option 2: Streamlit (with ngrok tunnel)

```python
# Cell 1: Install dependencies
!pip install streamlit google-api-python-client isodate langdetect pyngrok
!pip install transformers accelerate bitsandbytes

# Cell 2: Set API keys
import os
os.environ['YOUTUBE_API_KEY'] = 'YOUR_YOUTUBE_API_KEY'
os.environ['NGROK_AUTH_TOKEN'] = 'YOUR_NGROK_TOKEN'  # Get free token from ngrok.com

# Cell 3: Upload files
# Upload pipeline.py and app_streamlit.py using Colab file browser

# Cell 4: Run Streamlit with ngrok
import threading
from pyngrok import ngrok

ngrok.set_auth_token(os.environ.get('NGROK_AUTH_TOKEN', ''))

def run_streamlit():
    import os
    os.system('streamlit run app_streamlit.py --server.port 8501 --server.headless true')

thread = threading.Thread(target=run_streamlit)
thread.start()

import time
time.sleep(10)  # Wait for Streamlit to start

public_url = ngrok.connect(8501)
print(f"\nğŸŒ PUBLIC URL: {public_url}")
print("Share this URL for demo access!")
```

---

## ğŸ“ Project Structure

```
youtube_report_webapp_colab/
â”œâ”€â”€ app_streamlit.py    # Streamlit web app (use with ngrok)
â”œâ”€â”€ app_gradio.py       # Gradio web app (built-in share=True)
â”œâ”€â”€ pipeline.py         # Core pipeline with LLM support
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md          # This file
```

---

## ğŸ”§ LLM Modes

The application has two modes:

| Mode | Condition | Behavior |
|------|-----------|----------|
| **FULL LLM** | GPU available + model loads | Real AI-generated summaries using Llama 3.1-8B |
| **FALLBACK** | No GPU or load fails | Placeholder summaries with basic stats |

The UI clearly shows which mode is active:
- âœ… `LLM Status: FULL (Llama 3.1-8B on CUDA)` - Real AI analysis
- âš ï¸ `LLM Status: FALLBACK` - Placeholder mode

---

## âš™ï¸ Configurable Parameters

These can be adjusted in the sidebar (Streamlit) or accordion (Gradio):

| Parameter | Default | Range | Description |
|-----------|---------|-------|-------------|
| Report Language | English | 7 options | Output language for summaries |
| Max Comments | 100 | 10-200 | Comments to analyze |
| Quality Gate | On | Toggle | Validate and regenerate low-quality outputs |
| Min Summary Length | 100 | 50-400 | Minimum chars for valid summary |
| Max New Tokens | 512 | 128-1024 | Maximum LLM output tokens |
| Temperature | 0.7 | 0.1-1.5 | LLM creativity (lower=focused) |

---

## ğŸ”‘ Getting API Keys

### YouTube Data API Key
1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Create project â†’ Enable YouTube Data API v3
3. Create Credentials â†’ API Key
4. Copy and set: `os.environ['YOUTUBE_API_KEY'] = 'your-key'`

### ngrok Auth Token (for Streamlit only)
1. Sign up at [ngrok.com](https://ngrok.com/) (free)
2. Go to Dashboard â†’ Your Authtoken
3. Copy and set: `os.environ['NGROK_AUTH_TOKEN'] = 'your-token'`

---

## ğŸ“Š Example Output

The generated report includes:

```markdown
# YouTube Video Report

**Generated**: 2024-01-15 14:30:00
**LLM Status**: âœ… FULL (meta-llama/Llama-3.1-8B-Instruct on CUDA)

## Video Information
- Title: Amazing Video Title
- Channel: Cool Channel
- Duration: 12:34
...

## Video Summary

KEY POINT:
This video explores the fascinating world of...

DETAILED SUMMARY:
The creator takes viewers through an in-depth...

## Audience Reaction Summary

KEY POINT:
Viewers overwhelmingly praised the video's...

DETAILED SUMMARY:
The majority of comments (estimated 75%) express...

SENTIMENT BREAKDOWN:
Positive: 75%  Negative: 5%  Neutral: 20%
```

---

## ğŸ› ï¸ Troubleshooting

### "CUDA out of memory"
- Use 4-bit quantization (default: `use_4bit=True`)
- Reduce `max_new_tokens` to 256-384
- Restart Colab runtime to free GPU memory

### "Model loading takes forever"
- First load downloads ~4GB model files
- Subsequent runs are faster due to caching
- Ensure GPU runtime is selected (Runtime â†’ Change runtime type â†’ GPU)

### "LLM Status: FALLBACK"
- Check GPU: `!nvidia-smi`
- Check CUDA: `import torch; print(torch.cuda.is_available())`
- Install bitsandbytes: `!pip install bitsandbytes`

### "YouTube API Error"
- Verify API key is set correctly
- Check API quota (default: 10,000 units/day)
- Some videos have comments disabled

---

## ğŸ“ Technical Notes

- **Model**: `meta-llama/Llama-3.1-8B-Instruct` with 4-bit quantization
- **GPU Memory**: ~5GB with 4-bit quantization
- **Inference Time**: ~10-30 seconds per summary (depends on GPU)
- **Quality Gate**: Auto-regenerates outputs that fail validation

---

## ğŸ†š Comparison: Colab LLM vs. Web Edition

| Feature | Colab LLM Edition | Web Edition |
|---------|-------------------|-------------|
| LLM Support | âœ… Full (Llama 3.1-8B) | âš ï¸ Fallback only |
| GPU Required | âœ… Yes | âŒ No |
| Deployment | Temporary (ngrok/gradio.live) | Permanent (Streamlit Cloud) |
| Best For | Demos, research | Portfolio, always-on |

---

## ğŸ“„ License

MIT License

## ğŸ™ Acknowledgments

- Original pipeline: `merge_notebooks.ipynb` / `merge_notebooks.py`
- [Streamlit](https://streamlit.io/) & [Gradio](https://gradio.app/)
- [Hugging Face Transformers](https://huggingface.co/transformers)
- [Meta Llama](https://ai.meta.com/llama/)

---

**ğŸ¤– Powered by Llama 3.1-8B | Made for Colab GPU Demos**
